{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-17T10:22:39.218388Z",
     "iopub.status.busy": "2025-09-17T10:22:39.218156Z",
     "iopub.status.idle": "2025-09-17T10:22:39.233399Z",
     "shell.execute_reply": "2025-09-17T10:22:39.232672Z",
     "shell.execute_reply.started": "2025-09-17T10:22:39.218365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection success\n"
     ]
    }
   ],
   "source": [
    "import socket,warnings\n",
    "\n",
    "try:\n",
    "    socket.setdefaulttimeout(1)\n",
    "    socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect(('1.1.1.1', 53))\n",
    "    print(\"Connection success\")\n",
    "except socket.error as ex: raise Exception(\"STOP: No internet. Click '|<' in buttom right and set 'Internet' switch to on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T10:26:58.266129Z",
     "iopub.status.busy": "2025-09-17T10:26:58.265387Z",
     "iopub.status.idle": "2025-09-17T10:29:59.388802Z",
     "shell.execute_reply": "2025-09-17T10:29:59.387909Z",
     "shell.execute_reply.started": "2025-09-17T10:26:58.266104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mDone\n"
     ]
    }
   ],
   "source": [
    "# Upgrade pip\n",
    "!pip install -Uq --upgrade pip\n",
    "\n",
    "# Core libs (PyTorch + Accelerate)\n",
    "!pip install -Uq \"torch>=2.3.0\" \"accelerate>=0.34.0\"\n",
    "\n",
    "# Transformers + SentenceTransformers (compatible versions)\n",
    "!pip install -Uq \"transformers==4.44.2\" \"sentence-transformers==3.0.1\"\n",
    "\n",
    "# Supporting libs\n",
    "!pip install -Uq faiss-cpu psutil huggingface_hub langchain_community\n",
    "\n",
    "# Quantization support (8-bit / 4-bit)\n",
    "!pip install -Uq bitsandbytes\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T10:24:38.019169Z",
     "iopub.status.busy": "2025-09-17T10:24:38.018915Z",
     "iopub.status.idle": "2025-09-17T10:25:06.391505Z",
     "shell.execute_reply": "2025-09-17T10:25:06.390902Z",
     "shell.execute_reply.started": "2025-09-17T10:24:38.019147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 10:24:51.284978: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758104691.494569      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758104691.558338      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os, json, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T10:55:27.430239Z",
     "iopub.status.busy": "2025-09-17T10:55:27.429474Z",
     "iopub.status.idle": "2025-09-17T10:56:37.344300Z",
     "shell.execute_reply": "2025-09-17T10:56:37.343410Z",
     "shell.execute_reply.started": "2025-09-17T10:55:27.430204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089dd4da4db34230b29b9de2cb06ba1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6217108832ce470fb12830c9d1ed6a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RAG] Top-3 retrieval for: Strong Python skills and familiarity with AI/LLM concepts\n",
      "\n",
      "#1  score=0.5837 [user_bio] Software Enginner Trainee Python C C++ Go Generative AI Agentic AI Helsinki Finland Programming Python FastAPI C C++ Go idiomatic microservices AI LLMs Prompt Design RAG Deep Learning fine-tuning models DevOps Docker Kubernetes Git Linux Shell Scripting.\n",
      "#2  score=0.4425 [github_profile] Software Engineer - Generative AI & Agentic AI\n",
      "#3  score=0.3414 [repo_chess-ai-classifier] Export the trained model Inference python from fastai.vision.all import Load trained model learn load_learner chessman_model.pkl Predict on new image pred pred_idx probs learn.predict path to chess_piece.jpg print f Prediction pred Files - chess_ai_notebook.ipynb - Complete training and evaluation pipeline\n",
      "\n",
      "[RESULT] Generated CV Skills Section:\n",
      "\n",
      "Skills:\n",
      "- Strong Python programming language proficiency\n",
      "- Familiarity with AI/ML concepts including Large Language Models (LLMs)\n",
      "- Experience in designing prompts for Retrieval-Augmented Generation (RAG) applications\n"
     ]
    }
   ],
   "source": [
    "# --- Cache setup ---\n",
    "cache_dir = Path(\"/kaggle/working/cache\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "os.environ.setdefault(\"HF_HOME\", str(cache_dir / \"hf\"))\n",
    "os.environ.setdefault(\"TORCH_HOME\", str(cache_dir / \"torch\"))\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"2\")\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# --- Load HF token ---\n",
    "env_path = Path(\"/kaggle/input/env-vars/.env\")\n",
    "raw = env_path.read_text(encoding=\"utf-8\")\n",
    "m = re.search(r'hf_[A-Za-z0-9_-]+', raw)\n",
    "if not m:\n",
    "    raise RuntimeError(\"No hf_ token found in /kaggle/input/env-vars/.env\")\n",
    "HF_TOKEN = m.group(0)\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# --- Config ---\n",
    "MODEL_NAME     = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "EMBEDDER_NAME  = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "TOP_K          = 3\n",
    "MAX_NEW_TOKENS = 120\n",
    "INPUT_MAX_LEN  = 512\n",
    "DATA_DIR       = Path(\"/kaggle/input/data-resume\")\n",
    "\n",
    "# --- Require files ---\n",
    "for f in [\"embeddings.npy\", \"texts.json\", \"metadata.json\", \"faiss.index\"]:\n",
    "    p = DATA_DIR / f\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing {p}. Upload data to Kaggle dataset.\")\n",
    "\n",
    "embeddings = np.load(DATA_DIR / \"embeddings.npy\").astype(\"float32\")\n",
    "metadata = json.loads((DATA_DIR / \"metadata.json\").read_text())\n",
    "index = faiss.read_index(str(DATA_DIR / \"faiss.index\"))\n",
    "\n",
    "# Sentence embeddings\n",
    "embed_model = SentenceTransformer(EMBEDDER_NAME)\n",
    "\n",
    "# --- Tokenizer & model ---\n",
    "cuda = torch.cuda.is_available()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN, use_fast=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16 if cuda else torch.float32,\n",
    "    device_map=\"auto\" if cuda else None,\n",
    ")\n",
    "\n",
    "# --- Text generation pipeline ---\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# --- Helpers ---\n",
    "def _truncate_prompt(text: str) -> str:\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(ids) > INPUT_MAX_LEN:\n",
    "        ids = ids[-INPUT_MAX_LEN:]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def encode_query(q: str) -> np.ndarray:\n",
    "    return embed_model.encode([q], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "def rag_generate(query: str, k: int = TOP_K, show_scores: bool = True) -> str:\n",
    "    if not query or not isinstance(query, str):\n",
    "        raise ValueError(\"Query must be a non-empty string\")\n",
    "\n",
    "    q_emb = encode_query(query)\n",
    "    scores, idxs = index.search(q_emb, k=k)\n",
    "\n",
    "    if idxs.size == 0:\n",
    "        if show_scores:\n",
    "            print(\"[RAG] No hits found.\")\n",
    "        return \"Skills:\\n- (no relevant data found)\"\n",
    "\n",
    "    # Retrieval preview\n",
    "    if show_scores:\n",
    "        print(f\"\\n[RAG] Top-{k} retrieval for: {query}\\n\")\n",
    "        for rank, (i, s) in enumerate(zip(idxs[0], scores[0]), start=1):\n",
    "            if 0 <= i < len(metadata):\n",
    "                src, sent = metadata[i]\n",
    "                print(f\"#{rank:<2} score={s:0.4f} [{src}] {sent}\")\n",
    "        print()\n",
    "\n",
    "    # Build concise context, avoid code-y lines\n",
    "    lines = []\n",
    "    for i in idxs[0]:\n",
    "        if 0 <= i < len(metadata):\n",
    "            src, sent = metadata[i]\n",
    "            if not any(kw in sent.lower() for kw in [\"import\", \"```\", \".py\", \"print\"]):\n",
    "                lines.append(f\"{src}: {sent}\")\n",
    "    context = \"\\n\".join(lines[:2]) if lines else \"(empty)\"\n",
    "\n",
    "    # Dynamic prompt\n",
    "    prompt_template = (\n",
    "        \"Generate a CV Skills section tailored to the job requirements: \\\"{query}\\\"\\n\"\n",
    "        \"Use ONLY this data, DO NOT add unlisted details (e.g., years of experience, unlisted frameworks like TensorFlow), avoid markdown, and use exact terms from the data (e.g., RAG for Retrieval-Augmented Generation, FastAPI, FastAI, agentic AI):\\n\"\n",
    "        \"{context}\\n\"\n",
    "        \"Return 2-3 unique, relevant bullets matching the job requirements:\\n\"\n",
    "        \"Skills:\\n- \"\n",
    "    )\n",
    "    prompt = _truncate_prompt(prompt_template.format(context=context, query=query))\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        result = text_gen(prompt)\n",
    "        output = result[0][\"generated_text\"].strip()\n",
    "\n",
    "    # Extract bullets\n",
    "    skills_start = output.find(\"Skills:\")\n",
    "    if skills_start == -1:\n",
    "        return \"Skills:\\n- (no skills generated)\"\n",
    "    output = output[skills_start:]\n",
    "    bullets = [\n",
    "        line.strip()[2:] for line in output.split(\"\\n\")\n",
    "        if line.strip().startswith(\"- \") and len(line.strip()) > 5\n",
    "    ]\n",
    "    unique_bullets, seen = [], set()\n",
    "    for bullet in bullets:\n",
    "        skill = re.sub(r'[`*]', '', bullet.strip())  # Remove markdown\n",
    "        if skill and skill.lower() not in seen and len(unique_bullets) < 3:\n",
    "            unique_bullets.append(f\"- {skill}\")\n",
    "            seen.add(skill.lower())\n",
    "    if not unique_bullets:\n",
    "        return \"Skills:\\n- (no relevant skills generated)\"\n",
    "    return \"Skills:\\n\" + \"\\n\".join(unique_bullets[:3])\n",
    "\n",
    "def main():\n",
    "    query = \"Strong Python skills and familiarity with AI/LLM concepts\"\n",
    "    cv_skills = rag_generate(query, k=TOP_K, show_scores=True)\n",
    "    print(\"[RESULT] Generated CV Skills Section:\\n\")\n",
    "    print(cv_skills)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8288269,
     "sourceId": 13085799,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8288800,
     "sourceId": 13086554,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

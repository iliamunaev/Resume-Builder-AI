{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13085799,"sourceType":"datasetVersion","datasetId":8288269},{"sourceId":13086554,"sourceType":"datasetVersion","datasetId":8288800}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import socket,warnings\n\ntry:\n    socket.setdefaulttimeout(1)\n    socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect(('1.1.1.1', 53))\n    print(\"Connection success\")\nexcept socket.error as ex: raise Exception(\"STOP: No internet. Click '|<' in buttom right and set 'Internet' switch to on\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:22:39.218156Z","iopub.execute_input":"2025-09-17T10:22:39.218388Z","iopub.status.idle":"2025-09-17T10:22:39.233399Z","shell.execute_reply.started":"2025-09-17T10:22:39.218365Z","shell.execute_reply":"2025-09-17T10:22:39.232672Z"}},"outputs":[{"name":"stdout","text":"Connection success\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Upgrade pip\n!pip install -Uq --upgrade pip\n\n# Core libs (PyTorch + Accelerate)\n!pip install -Uq \"torch>=2.3.0\" \"accelerate>=0.34.0\"\n\n# Transformers + SentenceTransformers (compatible versions)\n!pip install -Uq \"transformers==4.44.2\" \"sentence-transformers==3.0.1\"\n\n# Supporting libs\n!pip install -Uq faiss-cpu psutil huggingface_hub langchain_community\n\n# Quantization support (8-bit / 4-bit)\n!pip install -Uq bitsandbytes\n\nprint(\"Done\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:26:58.265387Z","iopub.execute_input":"2025-09-17T10:26:58.266129Z","iopub.status.idle":"2025-09-17T10:29:59.388802Z","shell.execute_reply.started":"2025-09-17T10:26:58.266104Z","shell.execute_reply":"2025-09-17T10:29:59.387909Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mDone\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os, json, re\nfrom pathlib import Path\nimport numpy as np\nimport faiss\nimport torch\nfrom huggingface_hub import login\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\nprint(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:24:38.018915Z","iopub.execute_input":"2025-09-17T10:24:38.019169Z","iopub.status.idle":"2025-09-17T10:25:06.391505Z","shell.execute_reply.started":"2025-09-17T10:24:38.019147Z","shell.execute_reply":"2025-09-17T10:25:06.390902Z"}},"outputs":[{"name":"stderr","text":"2025-09-17 10:24:51.284978: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758104691.494569      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758104691.558338      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- Cache setup ---\ncache_dir = Path(\"/kaggle/working/cache\")\ncache_dir.mkdir(exist_ok=True)\nos.environ.setdefault(\"HF_HOME\", str(cache_dir / \"hf\"))\nos.environ.setdefault(\"TORCH_HOME\", str(cache_dir / \"torch\"))\nos.environ.setdefault(\"OMP_NUM_THREADS\", \"2\")\nos.environ.setdefault(\"MKL_NUM_THREADS\", \"2\")\nos.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# --- Load HF token ---\nenv_path = Path(\"/kaggle/input/env-vars/.env\")\nraw = env_path.read_text(encoding=\"utf-8\")\nm = re.search(r'hf_[A-Za-z0-9_-]+', raw)\nif not m:\n    raise RuntimeError(\"No hf_ token found in /kaggle/input/env-vars/.env\")\nHF_TOKEN = m.group(0)\nlogin(token=HF_TOKEN)\n\n# --- Config ---\nMODEL_NAME     = \"meta-llama/Llama-3.2-3B-Instruct\"\nEMBEDDER_NAME  = \"sentence-transformers/all-MiniLM-L6-v2\"\nTOP_K          = 3\nMAX_NEW_TOKENS = 120\nINPUT_MAX_LEN  = 512\nDATA_DIR       = Path(\"/kaggle/input/data-resume\")\n\n# --- Require files ---\nfor f in [\"embeddings.npy\", \"texts.json\", \"metadata.json\", \"faiss.index\"]:\n    p = DATA_DIR / f\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing {p}. Upload data to Kaggle dataset.\")\n\nembeddings = np.load(DATA_DIR / \"embeddings.npy\").astype(\"float32\")\nmetadata = json.loads((DATA_DIR / \"metadata.json\").read_text())\nindex = faiss.read_index(str(DATA_DIR / \"faiss.index\"))\n\n# Sentence embeddings\nembed_model = SentenceTransformer(EMBEDDER_NAME)\n\n# --- Tokenizer & model ---\ncuda = torch.cuda.is_available()\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN, use_fast=True)\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    token=HF_TOKEN,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16 if cuda else torch.float32,\n    device_map=\"auto\" if cuda else None,\n)\n\n# --- Text generation pipeline ---\ntext_gen = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=MAX_NEW_TOKENS,\n    do_sample=True,\n    top_k=50,\n    top_p=0.9,\n    repetition_penalty=1.2,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\n# --- Helpers ---\ndef _truncate_prompt(text: str) -> str:\n    ids = tokenizer.encode(text, add_special_tokens=False)\n    if len(ids) > INPUT_MAX_LEN:\n        ids = ids[-INPUT_MAX_LEN:]\n    return tokenizer.decode(ids, skip_special_tokens=True)\n\ndef encode_query(q: str) -> np.ndarray:\n    return embed_model.encode([q], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n\ndef rag_generate(query: str, k: int = TOP_K, show_scores: bool = True) -> str:\n    if not query or not isinstance(query, str):\n        raise ValueError(\"Query must be a non-empty string\")\n\n    q_emb = encode_query(query)\n    scores, idxs = index.search(q_emb, k=k)\n\n    if idxs.size == 0:\n        if show_scores:\n            print(\"[RAG] No hits found.\")\n        return \"Skills:\\n- (no relevant data found)\"\n\n    # Retrieval preview\n    if show_scores:\n        print(f\"\\n[RAG] Top-{k} retrieval for: {query}\\n\")\n        for rank, (i, s) in enumerate(zip(idxs[0], scores[0]), start=1):\n            if 0 <= i < len(metadata):\n                src, sent = metadata[i]\n                print(f\"#{rank:<2} score={s:0.4f} [{src}] {sent}\")\n        print()\n\n    # Build concise context, avoid code-y lines\n    lines = []\n    for i in idxs[0]:\n        if 0 <= i < len(metadata):\n            src, sent = metadata[i]\n            if not any(kw in sent.lower() for kw in [\"import\", \"```\", \".py\", \"print\"]):\n                lines.append(f\"{src}: {sent}\")\n    context = \"\\n\".join(lines[:2]) if lines else \"(empty)\"\n\n    # Dynamic prompt\n    prompt_template = (\n        \"Generate a CV Skills section tailored to the job requirements: \\\"{query}\\\"\\n\"\n        \"Use ONLY this data, DO NOT add unlisted details (e.g., years of experience, unlisted frameworks like TensorFlow), avoid markdown, and use exact terms from the data (e.g., RAG for Retrieval-Augmented Generation, FastAPI, FastAI, agentic AI):\\n\"\n        \"{context}\\n\"\n        \"Return 2-3 unique, relevant bullets matching the job requirements:\\n\"\n        \"Skills:\\n- \"\n    )\n    prompt = _truncate_prompt(prompt_template.format(context=context, query=query))\n\n    with torch.inference_mode():\n        result = text_gen(prompt)\n        output = result[0][\"generated_text\"].strip()\n\n    # Extract bullets\n    skills_start = output.find(\"Skills:\")\n    if skills_start == -1:\n        return \"Skills:\\n- (no skills generated)\"\n    output = output[skills_start:]\n    bullets = [\n        line.strip()[2:] for line in output.split(\"\\n\")\n        if line.strip().startswith(\"- \") and len(line.strip()) > 5\n    ]\n    unique_bullets, seen = [], set()\n    for bullet in bullets:\n        skill = re.sub(r'[`*]', '', bullet.strip())  # Remove markdown\n        if skill and skill.lower() not in seen and len(unique_bullets) < 3:\n            unique_bullets.append(f\"- {skill}\")\n            seen.add(skill.lower())\n    if not unique_bullets:\n        return \"Skills:\\n- (no relevant skills generated)\"\n    return \"Skills:\\n\" + \"\\n\".join(unique_bullets[:3])\n\ndef main():\n    query = \"Strong Python skills and familiarity with AI/LLM concepts\"\n    cv_skills = rag_generate(query, k=TOP_K, show_scores=True)\n    print(\"[RESULT] Generated CV Skills Section:\\n\")\n    print(cv_skills)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:27.429474Z","iopub.execute_input":"2025-09-17T10:55:27.430239Z","iopub.status.idle":"2025-09-17T10:56:37.344300Z","shell.execute_reply.started":"2025-09-17T10:55:27.430204Z","shell.execute_reply":"2025-09-17T10:56:37.343410Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089dd4da4db34230b29b9de2cb06ba1c"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6217108832ce470fb12830c9d1ed6a35"}},"metadata":{}},{"name":"stdout","text":"\n[RAG] Top-3 retrieval for: Strong Python skills and familiarity with AI/LLM concepts\n\n#1  score=0.5837 [user_bio] Software Enginner Trainee Python C C++ Go Generative AI Agentic AI Helsinki Finland Programming Python FastAPI C C++ Go idiomatic microservices AI LLMs Prompt Design RAG Deep Learning fine-tuning models DevOps Docker Kubernetes Git Linux Shell Scripting.\n#2  score=0.4425 [github_profile] Software Engineer - Generative AI & Agentic AI\n#3  score=0.3414 [repo_chess-ai-classifier] Export the trained model Inference python from fastai.vision.all import Load trained model learn load_learner chessman_model.pkl Predict on new image pred pred_idx probs learn.predict path to chess_piece.jpg print f Prediction pred Files - chess_ai_notebook.ipynb - Complete training and evaluation pipeline\n\n[RESULT] Generated CV Skills Section:\n\nSkills:\n- Strong Python programming language proficiency\n- Familiarity with AI/ML concepts including Large Language Models (LLMs)\n- Experience in designing prompts for Retrieval-Augmented Generation (RAG) applications\n","output_type":"stream"}],"execution_count":9}]}